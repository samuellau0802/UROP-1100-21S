{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a word2vec model using Keras\n",
    "[reference link](https://www.tensorflow.org/tutorials/text/word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import re\n",
    "import string\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "SEED = 42\n",
    "AUTOTUNE = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates skip-gram pairs with negative sampling for a list of sequences\n",
    "# (int-encoded sentences) based on window size, number of negative samples\n",
    "# and vocabulary size.\n",
    "def generate_training_data(sequences, window_size, num_ns, vocab_size, seed):\n",
    "  # Elements of each training example are appended to these lists.\n",
    "  targets, contexts, labels = [], [], []\n",
    "\n",
    "  # Build the sampling table for `vocab_size` tokens.\n",
    "  sampling_table = tf.keras.preprocessing.sequence.make_sampling_table(vocab_size)\n",
    "\n",
    "  # Iterate over all sequences (sentences) in the dataset.\n",
    "  for sequence in tqdm.tqdm(sequences):\n",
    "\n",
    "    # Generate positive skip-gram pairs for a sequence (sentence).\n",
    "    positive_skip_grams, _ = tf.keras.preprocessing.sequence.skipgrams(\n",
    "          sequence,\n",
    "          vocabulary_size=vocab_size,\n",
    "          sampling_table=sampling_table,\n",
    "          window_size=window_size,\n",
    "          negative_samples=0)\n",
    "\n",
    "    # Iterate over each positive skip-gram pair to produce training examples\n",
    "    # with a positive context word and negative samples.\n",
    "    for target_word, context_word in positive_skip_grams:\n",
    "      context_class = tf.expand_dims(\n",
    "          tf.constant([context_word], dtype=\"int64\"), 1)\n",
    "      negative_sampling_candidates, _, _ = tf.random.log_uniform_candidate_sampler(\n",
    "          true_classes=context_class,\n",
    "          num_true=1,\n",
    "          num_sampled=num_ns,\n",
    "          unique=True,\n",
    "          range_max=vocab_size,\n",
    "          seed=SEED,\n",
    "          name=\"negative_sampling\")\n",
    "\n",
    "      # Build context and label vectors (for one target word)\n",
    "      negative_sampling_candidates = tf.expand_dims(\n",
    "          negative_sampling_candidates, 1)\n",
    "\n",
    "      context = tf.concat([context_class, negative_sampling_candidates], 0)\n",
    "      label = tf.constant([1] + [0]*num_ns, dtype=\"int64\")\n",
    "\n",
    "      # Append each element from the training example to global lists.\n",
    "      targets.append(target_word)\n",
    "      contexts.append(context)\n",
    "      labels.append(label)\n",
    "\n",
    "  return targets, contexts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = r\"right.txt\"\n",
    "# Use the non empty lines to construct a tf.data.TextLineDataset object for the next step\n",
    "text_ds = tf.data.TextLineDataset(path_to_file).filter(lambda x: tf.cast(tf.strings.length(x), bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase the text and remove punctuation.\n",
    "def custom_standardization(input_data):\n",
    "  lowercase = tf.strings.lower(input_data)\n",
    "  return tf.strings.regex_replace(lowercase,\n",
    "                                  '[%s]' % re.escape(string.punctuation), '')\n",
    "\n",
    "\n",
    "# Define the vocabulary size and the number of words in a sequence.\n",
    "vocab_size = 8192\n",
    "sequence_length = 10\n",
    "\n",
    "# Use the `TextVectorization` layer to normalize, split, and map strings to\n",
    "# integers. Set the `output_sequence_length` length to pad all samples to the\n",
    "# same length.\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call TextVectorization.adapt on the text dataset to create vocabulary\n",
    "vectorize_layer.adapt(text_ds.batch(1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'to', 'of', 'and', 'a', 'in', 'that', 'is', 'for', 'on', 'it', 'as', 'he', '—', 'trump', 'with', 'was', 'his']\n"
     ]
    }
   ],
   "source": [
    "# Save the created vocabulary for reference.\n",
    "inverse_vocab = vectorize_layer.get_vocabulary()\n",
    "print(inverse_vocab[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the data in text_ds.\n",
    "text_vector_ds = text_ds.batch(1024).prefetch(AUTOTUNE).map(vectorize_layer).unbatch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33712\n"
     ]
    }
   ],
   "source": [
    "sequences = list(text_vector_ds.as_numpy_iterator())\n",
    "print(len(sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 99  16   1 166  44  13   1   4   2 238] => ['donald', 'trump', '[UNK]', 'bill', 'clinton', 'as', '[UNK]', 'of', 'the', 'great']\n",
      "[ 609 1666  482    3  593  155    2 5900 1643   64] => ['open', 'carry', 'comes', 'to', 'texas', 'why', 'the', 'lone', 'star', 'state']\n",
      "[ 157 1247    1  243  503  558  311  588    3    1] => ['gop', 'field', '[UNK]', 'obama’s', 'move', 'toward', 'executive', 'action', 'to', '[UNK]']\n",
      "[  46   71  471    3    1  163   46   71    9 5840] => ['president', 'obama', 'wants', 'to', '[UNK]', 'america', 'president', 'obama', 'is', 'plotting']\n",
      "[   1  433  751    3 1302  178   21 2446 3486 7795] => ['[UNK]', 'family', 'reports', 'to', 'prison', 'does', 'not', 'endorse', 'oregon', 'siege']\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences[:5]:\n",
    "  print(f\"{seq} => {[inverse_vocab[i] for i in seq]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33712/33712 [02:45<00:00, 203.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "targets.shape: (186450,)\n",
      "contexts.shape: (186450, 5)\n",
      "labels.shape: (186450, 5)\n"
     ]
    }
   ],
   "source": [
    "# sequences is now a list of int encoded sentences. \n",
    "# Just call the generate_training_data function defined earlier to generate training examples for the word2vec model. \n",
    "# To recap, the function iterates over each word from each sequence to collect positive and negative context words. \n",
    "# Length of target, contexts and labels should be the same, representing the total number of training examples.\n",
    "targets, contexts, labels = generate_training_data(\n",
    "    sequences=sequences,\n",
    "    window_size=2,\n",
    "    num_ns=4,\n",
    "    vocab_size=vocab_size,\n",
    "    seed=SEED)\n",
    "\n",
    "targets = np.array(targets)\n",
    "contexts = np.array(contexts)[:,:,0]\n",
    "labels = np.array(labels)\n",
    "\n",
    "print('\\n')\n",
    "print(f\"targets.shape: {targets.shape}\")\n",
    "print(f\"contexts.shape: {contexts.shape}\")\n",
    "print(f\"labels.shape: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=((TensorSpec(shape=(1024,), dtype=tf.int64, name=None), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None)), TensorSpec(shape=(1024, 5), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "# To perform efficient batching for the potentially large number of training examples, use the tf.data.Dataset API. \n",
    "# After this step, you would have a tf.data.Dataset object of (target_word, context_word), (label) elements to train your word2vec model\n",
    "BATCH_SIZE = 1024\n",
    "BUFFER_SIZE = 10000\n",
    "dataset = tf.data.Dataset.from_tensor_slices(((targets, contexts), labels))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset = dataset.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ns = 4\n",
    "class Word2Vec(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim):\n",
    "    super(Word2Vec, self).__init__()\n",
    "    self.target_embedding = layers.Embedding(vocab_size,\n",
    "                                      embedding_dim,\n",
    "                                      input_length=1,\n",
    "                                      name=\"w2v_embedding\")\n",
    "    self.context_embedding = layers.Embedding(vocab_size,\n",
    "                                       embedding_dim,\n",
    "                                       input_length=num_ns+1)\n",
    "\n",
    "  def call(self, pair):\n",
    "    target, context = pair\n",
    "    # target: (batch, dummy?)  # The dummy axis doesn't exist in TF2.7+\n",
    "    # context: (batch, context)\n",
    "    if len(target.shape) == 2:\n",
    "      target = tf.squeeze(target, axis=1)\n",
    "    # target: (batch,)\n",
    "    word_emb = self.target_embedding(target)\n",
    "    # word_emb: (batch, embed)\n",
    "    context_emb = self.context_embedding(context)\n",
    "    # context_emb: (batch, context, embed)\n",
    "    dots = tf.einsum('be,bce->bc', word_emb, context_emb)\n",
    "    # dots: (batch, context)\n",
    "    return dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model with the tf.keras.optimizers.Adam optimizer\n",
    "embedding_dim = 512\n",
    "word2vec = Word2Vec(vocab_size, embedding_dim)\n",
    "word2vec.compile(optimizer='adam',\n",
    "                 loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a callback to log training statistics for Tensorboard\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "182/182 [==============================] - 8s 13ms/step - loss: 1.4964 - accuracy: 0.4977\n",
      "Epoch 2/20\n",
      "182/182 [==============================] - 2s 13ms/step - loss: 1.1231 - accuracy: 0.6597\n",
      "Epoch 3/20\n",
      "182/182 [==============================] - 2s 12ms/step - loss: 0.8191 - accuracy: 0.7620\n",
      "Epoch 4/20\n",
      "182/182 [==============================] - 2s 11ms/step - loss: 0.6052 - accuracy: 0.8316\n",
      "Epoch 5/20\n",
      "182/182 [==============================] - 2s 11ms/step - loss: 0.4643 - accuracy: 0.8778\n",
      "Epoch 6/20\n",
      "182/182 [==============================] - 2s 10ms/step - loss: 0.3705 - accuracy: 0.9072\n",
      "Epoch 7/20\n",
      "182/182 [==============================] - 2s 11ms/step - loss: 0.3059 - accuracy: 0.9256\n",
      "Epoch 8/20\n",
      "182/182 [==============================] - 2s 10ms/step - loss: 0.2602 - accuracy: 0.9377\n",
      "Epoch 9/20\n",
      "182/182 [==============================] - 2s 11ms/step - loss: 0.2269 - accuracy: 0.9458\n",
      "Epoch 10/20\n",
      "182/182 [==============================] - 2s 11ms/step - loss: 0.2023 - accuracy: 0.9503\n",
      "Epoch 11/20\n",
      "182/182 [==============================] - 2s 11ms/step - loss: 0.1838 - accuracy: 0.9533\n",
      "Epoch 12/20\n",
      "182/182 [==============================] - 2s 11ms/step - loss: 0.1696 - accuracy: 0.9553\n",
      "Epoch 13/20\n",
      "182/182 [==============================] - 2s 11ms/step - loss: 0.1585 - accuracy: 0.9565\n",
      "Epoch 14/20\n",
      "182/182 [==============================] - 2s 11ms/step - loss: 0.1498 - accuracy: 0.9572\n",
      "Epoch 15/20\n",
      "182/182 [==============================] - 2s 11ms/step - loss: 0.1429 - accuracy: 0.9577\n",
      "Epoch 16/20\n",
      "182/182 [==============================] - 2s 10ms/step - loss: 0.1373 - accuracy: 0.9579\n",
      "Epoch 17/20\n",
      "182/182 [==============================] - 2s 10ms/step - loss: 0.1327 - accuracy: 0.9582\n",
      "Epoch 18/20\n",
      "182/182 [==============================] - 2s 9ms/step - loss: 0.1289 - accuracy: 0.9583\n",
      "Epoch 19/20\n",
      "182/182 [==============================] - 2s 10ms/step - loss: 0.1257 - accuracy: 0.9584\n",
      "Epoch 20/20\n",
      "182/182 [==============================] - 2s 9ms/step - loss: 0.1230 - accuracy: 0.9585\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20a8ebfd4c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model on the dataset for some number of epochs:\n",
    "word2vec.fit(dataset, epochs=20, callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = word2vec.get_layer('w2v_embedding').get_weights()[0]\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.index('trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04183096, -0.14334643,  0.08857907, -0.20667455, -0.1791839 ,\n",
       "        0.02745217, -0.07978299, -0.19490163,  0.20630714,  0.01948943,\n",
       "       -0.18460403, -0.1077231 ,  0.2540952 , -0.04908137,  0.07910471,\n",
       "        0.04107846, -0.0042479 , -0.00710103,  0.02440254, -0.27768475,\n",
       "        0.08844206,  0.33321202, -0.22374986,  0.03017825,  0.16559571,\n",
       "       -0.25876373, -0.15889056,  0.4555668 , -0.20036584,  0.00602072,\n",
       "        0.02155547, -0.11949037,  0.2183996 ,  0.09545062, -0.14154981,\n",
       "       -0.02461464,  0.2817713 ,  0.08902241, -0.17498803,  0.15320791,\n",
       "       -0.10299405, -0.05487236, -0.04593365,  0.09375723, -0.13382937,\n",
       "        0.1609062 , -0.10219578, -0.19882713,  0.15592232, -0.00599622,\n",
       "       -0.31998628,  0.37073052, -0.128623  , -0.02695661, -0.08180618,\n",
       "        0.10873813, -0.06596952,  0.36303002,  0.05935183,  0.11171316,\n",
       "       -0.18485136, -0.02479523, -0.1867881 , -0.24616995, -0.04175721,\n",
       "       -0.09527916,  0.08593529,  0.31076223,  0.18801558,  0.17201614,\n",
       "       -0.3308616 , -0.01915827,  0.12653026,  0.19545695, -0.1336944 ,\n",
       "       -0.25385585,  0.03425837, -0.13209307,  0.14679976, -0.16723536,\n",
       "        0.16824989,  0.01102608, -0.04598646, -0.02490333,  0.01166305,\n",
       "       -0.23974623,  0.25986034, -0.05051304, -0.23617841, -0.28602475,\n",
       "       -0.32608768, -0.23444313, -0.31028867,  0.24190661, -0.09167337,\n",
       "       -0.15802872, -0.08916152, -0.09747252,  0.15511015, -0.15864186,\n",
       "       -0.38669583,  0.05699906,  0.10233024, -0.11401962,  0.10680138,\n",
       "        0.18771364,  0.19610085,  0.01552271, -0.13444889,  0.0397345 ,\n",
       "       -0.21324858, -0.41566885, -0.25163448,  0.04633881, -0.17538433,\n",
       "        0.11808319,  0.10869493, -0.00502412,  0.38191667,  0.06523404,\n",
       "       -0.2310771 ,  0.01744885, -0.18240519, -0.01439309,  0.15645187,\n",
       "        0.1433    , -0.04637133, -0.10321338, -0.32591087, -0.22692458,\n",
       "        0.12593679,  0.20570906,  0.27822623,  0.17122169, -0.15702564,\n",
       "        0.09980594,  0.04692267, -0.32888594, -0.26872703,  0.21796936,\n",
       "       -0.19088478, -0.02925099,  0.02107798,  0.03462363,  0.37156916,\n",
       "       -0.07725835,  0.06224125, -0.09242782,  0.0809727 , -0.31694272,\n",
       "        0.09589408, -0.10894576,  0.13600294,  0.07959639,  0.32616314,\n",
       "       -0.05060598, -0.01173325, -0.0505052 ,  0.07565963,  0.06185925,\n",
       "       -0.1331976 ,  0.08123262,  0.23907049,  0.03709733,  0.2629534 ,\n",
       "       -0.15683952, -0.25558102,  0.09493272, -0.22321576,  0.3275403 ,\n",
       "       -0.08259142, -0.02552617, -0.06466751, -0.16958442,  0.40125147,\n",
       "       -0.25040823, -0.02833074,  0.24375713,  0.26569045, -0.02678745,\n",
       "       -0.09934137,  0.2015993 ,  0.21350077, -0.06148186, -0.15890552,\n",
       "       -0.0674368 ,  0.08236782,  0.1782508 ,  0.28145084, -0.48020998,\n",
       "       -0.1745389 , -0.08733069,  0.19803488, -0.00545422,  0.11416087,\n",
       "       -0.09527592,  0.18518917, -0.01442352, -0.16837183,  0.08967624,\n",
       "        0.04732349,  0.20118769, -0.02970025,  0.17191865, -0.19130301,\n",
       "        0.00469392, -0.07240677, -0.19284436, -0.28024867,  0.12226877,\n",
       "       -0.05219123, -0.15309623, -0.04853363, -0.00460932,  0.26277667,\n",
       "        0.02022794, -0.20388912, -0.04229781, -0.24794936,  0.01987172,\n",
       "        0.0409303 , -0.03229134, -0.03372125, -0.13856909, -0.0648692 ,\n",
       "        0.1375206 ,  0.26019055,  0.19295377,  0.21385442, -0.29661813,\n",
       "       -0.20855524,  0.08260343, -0.04703938,  0.04352991, -0.00945006,\n",
       "        0.1217517 ,  0.15235233, -0.0469038 ,  0.42275354, -0.25314894,\n",
       "        0.09276441, -0.16806956,  0.11128848, -0.08450298,  0.1292412 ,\n",
       "        0.07403579, -0.05061388, -0.15407877,  0.3164378 ,  0.16094704,\n",
       "       -0.05344174,  0.02698195, -0.0463882 ,  0.1256107 ,  0.28047386,\n",
       "       -0.02990653,  0.12254819,  0.05842398,  0.04482367, -0.22183053,\n",
       "       -0.28198513, -0.0090616 ,  0.1634536 , -0.14817137, -0.29767936,\n",
       "        0.23986885, -0.20146036,  0.2536659 , -0.24138084, -0.05707493,\n",
       "       -0.28928605,  0.37169603, -0.08214403, -0.11954103, -0.0261733 ,\n",
       "        0.10251874, -0.18821236, -0.09714878,  0.15746816, -0.1428987 ,\n",
       "        0.02868167,  0.32032332,  0.16660029, -0.1567364 , -0.06701935,\n",
       "       -0.02958989, -0.0900244 , -0.14967573,  0.2665297 , -0.00863023,\n",
       "       -0.07143525,  0.06562498, -0.00720913, -0.09314895, -0.04490433,\n",
       "       -0.21271628, -0.15433459,  0.40386522, -0.17243001,  0.2667039 ,\n",
       "        0.06336801, -0.0320203 ,  0.1407467 , -0.08766314,  0.23929007,\n",
       "        0.07532632,  0.02132152,  0.07907804,  0.20614943,  0.28820312,\n",
       "        0.20521858,  0.42143443,  0.25139576, -0.13737704, -0.18269867,\n",
       "       -0.22620212,  0.07082438, -0.10357451, -0.2270836 ,  0.4567864 ,\n",
       "       -0.0704342 , -0.01124792, -0.18503919, -0.17607239, -0.09807706,\n",
       "       -0.08727825, -0.2101636 , -0.08295176,  0.09286394, -0.16666515,\n",
       "       -0.02437641, -0.18847397, -0.06036552, -0.10333444, -0.28645533,\n",
       "       -0.48624852, -0.3935561 ,  0.13290015,  0.03339751, -0.23902756,\n",
       "       -0.14503747, -0.12013622, -0.335136  ,  0.02593307, -0.3591985 ,\n",
       "        0.09294277,  0.1236888 , -0.12932143,  0.08085964, -0.01430598,\n",
       "        0.072182  ,  0.11003527, -0.1832345 ,  0.23977976, -0.15897168,\n",
       "        0.26206785,  0.12066662, -0.11611473,  0.2488425 ,  0.25370303,\n",
       "        0.06918614, -0.1559491 ,  0.24283749,  0.21946909, -0.21585265,\n",
       "        0.26846918, -0.06205476, -0.04291683, -0.01618931, -0.3851085 ,\n",
       "        0.01249914,  0.02997121, -0.25378895,  0.26906845, -0.30551118,\n",
       "       -0.08733459,  0.08426413, -0.09047046, -0.23713541,  0.12802732,\n",
       "       -0.01975669, -0.07925958,  0.03756934,  0.11157317, -0.04718487,\n",
       "       -0.1940948 ,  0.22301511,  0.08012734,  0.12092513,  0.07645285,\n",
       "        0.17288396, -0.10582491,  0.35802776, -0.05272228,  0.2600975 ,\n",
       "        0.40807778, -0.24925086,  0.06303806,  0.08675079, -0.25872833,\n",
       "       -0.14986028, -0.18885623, -0.05537278,  0.34379327, -0.00582143,\n",
       "        0.20533381, -0.3780379 ,  0.12834816, -0.2765933 ,  0.23564878,\n",
       "        0.03149001, -0.17945321, -0.34953222, -0.05643081,  0.04250976,\n",
       "       -0.2508998 ,  0.27177915, -0.13388617, -0.16884641,  0.08735335,\n",
       "        0.11910127,  0.08336799,  0.1605236 ,  0.13018921,  0.26266   ,\n",
       "       -0.00877272,  0.00238419, -0.22303753,  0.07363111,  0.09752139,\n",
       "        0.18952411, -0.30885446,  0.17679588, -0.09733541, -0.33062735,\n",
       "        0.14376576, -0.25076628,  0.1924712 , -0.03251034, -0.01827646,\n",
       "       -0.17126398,  0.06780824, -0.05291839,  0.02056063,  0.0589299 ,\n",
       "        0.0478675 ,  0.00322942,  0.00767404, -0.10662317,  0.0118938 ,\n",
       "       -0.12763575, -0.02415566,  0.10562683, -0.17718945,  0.01945756,\n",
       "        0.20111382, -0.23958887, -0.1470931 ,  0.03890755,  0.07600442,\n",
       "        0.1047014 , -0.37165073, -0.16455252,  0.24406612, -0.15362896,\n",
       "       -0.1321401 ,  0.31266233, -0.15427592, -0.07665985,  0.2217676 ,\n",
       "        0.26755905,  0.09260169,  0.13337049,  0.06734064,  0.25591484,\n",
       "       -0.04535272, -0.11050349,  0.18689035, -0.28309092, -0.14594255,\n",
       "       -0.14429963, -0.32037675, -0.19609101, -0.201241  , -0.3015534 ,\n",
       "        0.09441479, -0.07723355,  0.103054  ,  0.13517477,  0.16227002,\n",
       "       -0.00413258, -0.21517849,  0.17698692, -0.02211914,  0.2749063 ,\n",
       "       -0.07228931, -0.26002163,  0.02815965,  0.01560748,  0.12876542,\n",
       "       -0.10508785,  0.2538559 ,  0.08457367,  0.13137184, -0.13994636,\n",
       "       -0.04435543, -0.02525913,  0.22014217,  0.10731238,  0.0344355 ,\n",
       "        0.2839824 ,  0.2199277 ], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[vocab.index('trump')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and save the vectors and metadata files\n",
    "out_v = io.open('right_vectors.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('right_metadata.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "  if index == 0:\n",
    "    continue  # skip 0, it's padding.\n",
    "  vec = weights[index]\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "  out_m.write(word + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f00daf49d657fcaf7bdc5adbb3b43841207e448605290b2bd26bf112b8cca0df"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('urop')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
